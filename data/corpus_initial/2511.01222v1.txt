Title: Perturbed Double Machine Learning: Nonstandard Inference Beyond the Parametric Length
URL: http://arxiv.org/abs/2511.01222v1
Published: 2025-11-03 04:46:03+00:00
----------------------------------------
We study inference on a low dimensional functional $β$ in the presence of possibly infinite dimensional nuisance parameters. Classical inferential methods are typically based on the Wald interval, whose large sample validity rests on the asymptotic negligibility of the nuisance error; for example, estimators based on the influence curve of the parameter (Double/Debiased Machine Learning DML estimators) are asymptotically Gaussian when the nuisance estimators converge at rates faster than $n^{-1/4}$. Although, under suitable conditions, such negligibility can hold even in nonparametric classes, it can be restrictive. To relax this requirement, we propose Perturbed Double Machine Learning (Perturbed DML) to ensure valid inference even when nuisance estimators converge at rates slower than $n^{-1/4}$. Our proposal is to 1) inject randomness into the nuisance estimation step to generate a collection of perturbed nuisance models, each yielding an estimate of $β$ and a corresponding Wald interval, and 2) filter out perturbations whose deviations from the original DML estimate exceed a threshold. For Lasso nuisance learners, we show that, with high probability, at least one perturbation produces nuisance estimates sufficiently close to the truth, so that the associated estimator of $β$ is close to an oracle estimator with knowledge of the true nuisances. Taking the union of the retained intervals delivers valid coverage even when the DML estimator converges more slowly than $n^{-1/2}$. The framework extends to general machine learning nuisance learners, and simulations show that Perturbed DML can have coverage when state of the art methods fail.