Title: Spectral-Convergent Decentralized Machine Learning: Theory and Application in Space Networks
URL: http://arxiv.org/abs/2511.03291v1
Published: 2025-11-05 08:43:33+00:00
----------------------------------------
Decentralized machine learning (DML) supports collaborative training in large-scale networks with no central server. It is sensitive to the quality and reliability of inter-device communications that result in time-varying and stochastic topologies. This paper studies the impact of unreliable communication on the convergence of DML and establishes a direct connection between the spectral properties of the mixing process and the global performance. We provide rigorous convergence guarantees under random topologies and derive bounds that characterize the impact of the expected mixing matrix's spectral properties on learning. We formulate a spectral optimization problem that minimizes the spectral radius of the expected second-order mixing matrix to enhance the convergence rate under probabilistic link failures. To solve this non-smooth spectral problem in a fully decentralized manner, we design an efficient subgradient-based algorithm that integrates Chebyshev-accelerated eigenvector estimation with local update and aggregation weight adjustment, while ensuring symmetry and stochasticity constraints without central coordination. Experiments on a realistic low Earth orbit (LEO) satellite constellation with time-varying inter-satellite link models and real-world remote sensing data demonstrate the feasibility and effectiveness of the proposed method. The method significantly improves classification accuracy and convergence efficiency compared to existing baselines, validating its applicability in satellite and other decentralized systems.