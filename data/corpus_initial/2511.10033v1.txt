Title: 2.5D Transformer: An Efficient 3D Seismic Interpolation Method without Full 3D Training
URL: http://arxiv.org/abs/2511.10033v1
Published: 2025-11-13 07:15:07+00:00
----------------------------------------
Transformer has emerged as a powerful deep-learning technique for two-dimensional (2D) seismic data interpolation, owing to its global modeling ability. However, its core operation introduces heavy computational burden due to the quadratic complexity, hindering its further application to higher-dimensional data. To achieve Transformer-based three-dimensional (3D) seismic interpolation, we propose a 2.5-dimensional Transformer network (T-2.5D) that adopts a cross-dimensional transfer learning (TL) strategy, so as to adapt the 2D Transformer encoders to 3D seismic data. The proposed T-2.5D is mainly composed of 2D Transformer encoders and 3D seismic dimension adapters (SDAs). Each 3D SDA is placed before a Transformer encoder to learn spatial correlation information across seismic lines. The proposed cross-dimensional TL strategy comprises two stages: 2D pre-training and 3D fine-tuning. In the first stage, we optimize the 2D Transformer encoders using a large amount of 2D data patches. In the second stage, we freeze the 2D Transformer encoders and fine-tune the 3D SDAs using limited 3D data volumes. Extensive experiments on multiple datasets are conducted to assess the effectiveness and efficiency of T-2.5D. Experimental results demonstrate that the proposed method achieves comparable performance to that of full 3D Transformer at a significantly low cost.